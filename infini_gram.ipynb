{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zGU7snMz80vY",
        "6MkdMhOYZAra"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMTSxR7VTouv15+QzNaKXrv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grantinator/colab/blob/main/infini_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset (names dataset)"
      ],
      "metadata": {
        "id": "wIa9t0dQLuJ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPTQMS2tLoV-",
        "outputId": "e557fa37-0926-4013-875e-9a9abf83c621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-24 18:28:48--  https://raw.githubusercontent.com/exanova-y/von_neumann_dataset/refs/heads/main/biography.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 747769 (730K) [text/plain]\n",
            "Saving to: ‘corpus.txt’\n",
            "\n",
            "corpus.txt          100%[===================>] 730.24K  2.89MB/s    in 0.2s    \n",
            "\n",
            "2025-07-24 18:28:50 (2.89 MB/s) - ‘corpus.txt’ saved [747769/747769]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/exanova-y/von_neumann_dataset/refs/heads/main/biography.txt -O corpus.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "raw_corpus = open(\"corpus.txt\", \"r\").read()"
      ],
      "metadata": {
        "id": "Vm_E9cdlNmUr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Corpus:\n",
        "  def __init__(self, raw_corpus):\n",
        "    self._init_from_raw(raw_corpus)\n",
        "\n",
        "  def _init_from_raw(self, raw_corpus):\n",
        "    self.raw_corpus = raw_corpus\n",
        "    # maps nth word : index in self.corpus that slices upto and including that word.\n",
        "    self.word_end_index = {}\n",
        "    self.corpus = self._clean(raw_corpus)\n",
        "    self.words = self.corpus.split(' ')\n",
        "    self.vocab = set(self.words) | {\"<UNK>\"}\n",
        "\n",
        "  def get_words(self):\n",
        "    return self.words\n",
        "\n",
        "  def get_vocab(self):\n",
        "    return self.vocab\n",
        "\n",
        "  def get_vocab_size(self):\n",
        "    return len(self.vocab)\n",
        "\n",
        "  def get_corpus(self):\n",
        "    return self.corpus\n",
        "\n",
        "  def truncate_to(self, n_words):\n",
        "    truncated_raw = ' '.join(self.get_words()[:n_words]) # already cleaned from initial init.\n",
        "    self._init_from_raw(truncated_raw)\n",
        "\n",
        "    return self\n",
        "\n",
        "  def _clean(self, raw_corpus):\n",
        "    tokens = []\n",
        "    # Break into tokens\n",
        "    for line in raw_corpus.splitlines():\n",
        "      if len(line) == 0:\n",
        "        continue\n",
        "\n",
        "      line = line.split(' ')\n",
        "      tokens.extend(line)\n",
        "\n",
        "    # Clean/normalize individual tokens\n",
        "    cleaned_tokens = []\n",
        "    for i, token in enumerate(tokens):\n",
        "      token = token.lower()\n",
        "      token = token.strip()\n",
        "      # Strip punctuation\n",
        "      token = re.sub(r'[^a-zA-Z]', '', token)\n",
        "\n",
        "      if len(token) > 0:\n",
        "        cleaned_tokens.append(token)\n",
        "\n",
        "    for i, token in enumerate(cleaned_tokens):\n",
        "      if i == 0:\n",
        "        self.word_end_index[i] = len(token) - 1\n",
        "      else:\n",
        "        self.word_end_index[i] = self.word_end_index[i - 1] + len(token) + 1 # count space inbetween.\n",
        "\n",
        "    return ' '.join(cleaned_tokens)\n",
        "\n",
        "  def get_first_n_words(self, n):\n",
        "    return self.corpus[:self.word_end_index[n]+1]"
      ],
      "metadata": {
        "id": "63qh8dgAgYJb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Corpus class\n",
        "* Allows you to pull out the first n tokens (words).\n"
      ],
      "metadata": {
        "id": "kRE0gnaeLzNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = Corpus(raw_corpus)\n",
        "corpus.get_first_n_words(7)\n",
        "corpus.get_vocab_size()\n",
        "corpus.truncate_to(1000)"
      ],
      "metadata": {
        "id": "wClhbF53Lyuo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4da256a-8485-42e5-8849-771d8e93dc7b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Corpus at 0x7ecbc9063010>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = corpus.get_words()\n",
        "vocab = corpus.get_vocab()\n",
        "word_to_ix = {w: i for i, w in enumerate(vocab)}\n",
        "# word_to_ix['<UNK>'] = len(vocab)\n",
        "ix_to_word = {i: w for w, i in word_to_ix.items()}"
      ],
      "metadata": {
        "id": "Ho5DccTT8I0h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SuffixArray class\n",
        "* Builds the suffix arrays by mapping `[(suffix, startIndex)]`\n",
        "* For an ngram, finds all (suffix, startIndex) pars where the suffix begins with the given ngram."
      ],
      "metadata": {
        "id": "prBkKEYLu7Qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SuffixArray:\n",
        "  def __init__(self, corpus):\n",
        "    self.corpus = corpus\n",
        "    self.text = self.corpus.get_corpus()\n",
        "    self.suffixes = []\n",
        "    self._build()\n",
        "\n",
        "  def _build(self):\n",
        "    words = self.corpus.get_words()\n",
        "    suffixes_with_index = []\n",
        "    suffixIndex = len(self.corpus.get_corpus())\n",
        "    # i points to len(words) (end of string no trailing space)\n",
        "    # i - len(word) points to start of word\n",
        "    # i -len(word) + 1 poitns to end of next word\n",
        "    for word in words[::-1]:\n",
        "      suffixIndex -= len(word)\n",
        "      suffixes_with_index.append(suffixIndex)\n",
        "      suffixIndex -= 1 # subtrack word to move past this space.\n",
        "\n",
        "    # Sort on the suffix\n",
        "    sorted_suffixes = sorted(suffixes_with_index, key=lambda i: self.text[i:])\n",
        "    self.suffixes = sorted_suffixes\n",
        "\n",
        "  def find_ngram_occurrences(self, ngram):\n",
        "    \"\"\"\n",
        "    Find all suffixes that start with this prefix. Basically a lexicographical search\n",
        "    over suffixes.\n",
        "    \"\"\"\n",
        "    ngram = ngram.lower()\n",
        "    suffixes = self.suffixes # starting indexes\n",
        "\n",
        "    def getLowerBound():\n",
        "      low, high = 0, len(suffixes) - 1\n",
        "\n",
        "      while low <= high:\n",
        "        mid = (low + high) // 2\n",
        "\n",
        "        midSuffix = self.text[suffixes[mid]:]\n",
        "\n",
        "        if midSuffix < ngram:\n",
        "          low = mid + 1\n",
        "        else:\n",
        "          high = mid - 1\n",
        "      return low\n",
        "\n",
        "    def getUpperBound():\n",
        "      low, high = 0, len(suffixes) - 1\n",
        "      # Hack from chatgpt. If we just use ngram in the high search then ngram = \"von neumann\"\n",
        "      # we would not count \"von neumann made\" as a match. But instead we add the max ascii char\n",
        "      # so \"von neuamnn + <anything>\" < high_ngram and is counted.\n",
        "      highNgram = ngram + chr(255)\n",
        "\n",
        "      while low <= high:\n",
        "        mid = (low + high) // 2\n",
        "\n",
        "        midSuffix = self.text[suffixes[mid]:]\n",
        "\n",
        "        if midSuffix < highNgram:\n",
        "          low = mid + 1\n",
        "        else:\n",
        "          high = mid - 1\n",
        "      return low\n",
        "\n",
        "    upperBound = getUpperBound()\n",
        "    lowerBound = getLowerBound()\n",
        "    return suffixes[lowerBound:upperBound]"
      ],
      "metadata": {
        "id": "rSNB2jmzOT3S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## InfiniGram class\n",
        "* Returns a Counter for next word given a prefix.\n"
      ],
      "metadata": {
        "id": "XgGahTn1vV8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InfiniGram:\n",
        "  def __init__(self, corpus):\n",
        "    self.corpus = corpus\n",
        "    self.suffixArray = SuffixArray(self.corpus)\n",
        "\n",
        "  def get_next_word(self, prefix, suffixIndex):\n",
        "    corpus_text = self.corpus.get_corpus()\n",
        "    if not corpus_text[suffixIndex:].lstrip().startswith(prefix):\n",
        "        return None\n",
        "\n",
        "    # Move to the end of the current suffix.\n",
        "    i = suffixIndex + len(prefix)\n",
        "    while i < len(corpus_text) and corpus_text[i] != ' ':\n",
        "      i += 1\n",
        "\n",
        "    # i points to the space ending the suffix word now.\n",
        "    i += 1\n",
        "\n",
        "    # Eat up the next word to return.\n",
        "    j = i\n",
        "    while j < len(corpus_text) and corpus_text[j] != ' ':\n",
        "      j += 1\n",
        "\n",
        "\n",
        "    return corpus_text[i:j]\n",
        "\n",
        "  def predict_next(self, prefix):\n",
        "    \"\"\"\n",
        "    Returns a distribution across the entire vocab.\n",
        "    \"\"\"\n",
        "    suffixes = self.suffixArray.find_ngram_occurrences(prefix)\n",
        "\n",
        "    candidates = []\n",
        "\n",
        "    for startIndex in suffixes:\n",
        "      nextWord = self.get_next_word(prefix, startIndex)\n",
        "\n",
        "      if nextWord:\n",
        "        candidates.append(nextWord)\n",
        "\n",
        "    distribution = [0] * self.corpus.get_vocab_size()\n",
        "    counts = Counter(candidates)\n",
        "\n",
        "    total = sum(counts.values())\n",
        "\n",
        "    for word, count in counts.items():\n",
        "      distribution[word_to_ix[word]] = count / total\n",
        "\n",
        "    return torch.tensor(distribution)\n",
        "\n",
        "  def predict_next_word(self, prefix, top_k=5):\n",
        "    predictions = self.predict_next(prefix)\n",
        "\n",
        "    indices = torch.topk(predictions, top_k).indices\n",
        "    return [ix_to_word[i.item()] for i in indices]"
      ],
      "metadata": {
        "id": "8o4nSck2qP4Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ig = InfiniGram(corpus)"
      ],
      "metadata": {
        "id": "2NdAJj9mr3SF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3\n",
        "print(f\"Distribution across all words in vocab {ig.predict_next('von')[:10]}\")\n",
        "print(f\"Predict next {k} words {ig.predict_next_word('the', top_k=k)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LCtc_8iKTss",
        "outputId": "bf85ac30-a852-4f73-a0d5-68fd1baa2e4b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution across all words in vocab tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Predict next 3 words ['manhattan', 'worlds', 'same']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP *Impl*"
      ],
      "metadata": {
        "id": "tr4JgA3mYv3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "\n",
        "  def __init__(self, vocab_size, n_embd, n_hidden, block_size):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.n_embd = n_embd # dimensionality of each char's embedding\n",
        "    self.n_hidden = n_hidden # the number of neurons in the hidden layer\n",
        "    self.block_size = block_size # number of words in input layer\n",
        "    self.C = torch.randn((vocab_size + 1, n_embd)) # NOTE: vocab_size + 1 for the <UNK> word.\n",
        "    self.layers = [\n",
        "        Linear(n_embd * block_size, n_hidden), Tanh(),\n",
        "        Linear(n_hidden, n_hidden), Tanh(),\n",
        "        Linear(n_hidden, n_hidden), Tanh(),\n",
        "        Linear(n_hidden, n_hidden), Tanh(),\n",
        "        Linear(n_hidden, n_hidden), Tanh(),\n",
        "        Linear(n_hidden, vocab_size)\n",
        "    ]\n",
        "    self.parameters = [p for layer in self.layers for p in layer.parameters()]\n",
        "    self.set_up()\n",
        "\n",
        "\n",
        "  def set_up(self):\n",
        "    with torch.no_grad():\n",
        "      # last layer: make it less confident (for initialization make it closer to uniform distribution)\n",
        "      self.layers[-1].weight *= 0.1\n",
        "      for layer in self.layers[:-1]:\n",
        "        if isinstance(layer, Linear):\n",
        "          layer.weight *= (5/3) #(5/3) # kaiming init to make X @ W activations more unit normal at start\n",
        "\n",
        "    for p in self.parameters:\n",
        "      p.requires_grad = True\n",
        "\n",
        "  def forward(self, Xb, y):\n",
        "    emb = self.C[Xb]\n",
        "    emb.view(emb.shape[0], -1)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    return F.cross_entropy(x, y)\n",
        "\n",
        "  def predict(self, x):\n",
        "    with torch.no_grad():\n",
        "      emb = self.C[x]\n",
        "      emb.view(emb.shape[0], -1)\n",
        "      x = emb\n",
        "      for layer in self.layers:\n",
        "        # print(f\"Multiplying x@W {x.shape} x {layer.weight.shape}\")\n",
        "        try:\n",
        "          x = layer(x)\n",
        "        except Exception as e:\n",
        "          print(f\"Exception:\\n {e}\")\n",
        "\n",
        "      return F.softmax(x, dim=1).squeeze(0)"
      ],
      "metadata": {
        "id": "YUzyrhhzYmd3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(dataset):\n",
        "  X, Y = [], []\n",
        "  context = [0] * block_size\n",
        "  for i in range(len(dataset) - block_size):\n",
        "    context = [word_to_ix[w] for w in dataset[i:i+block_size]]\n",
        "    y = word_to_ix[dataset[i+block_size]]\n",
        "    X.append(context)\n",
        "    Y.append(y)\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  return X,Y"
      ],
      "metadata": {
        "id": "tToVfYidU2K-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer definitions"
      ],
      "metadata": {
        "id": "zGU7snMz80vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # khaming init or whatever its called. Prescale the weights so that when we do X @ W the activations are unit normal.\n",
        "    self.bias = torch.zeros(fan_out) if bias else None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight\n",
        "    if not self.bias is None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "class BatchNorm1d:\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # parameters (trained) as part of gamma * batchNorm + beta to be learned\n",
        "    # Start at 1 and zero so initialized values are unit normal but gamma and beta\n",
        "    # can be learned away from forcing activations to unit normal\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "    # running mean and variance for predictions\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate forward pass\n",
        "    if self.training:\n",
        "      xmean = x.mean(0, keepdim=True) # mean for each neuron across all examples\n",
        "      xvar = x.var(0, keepdim=True)\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # batch norm formula [https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html]\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    # Update running mean/std if in traning mode\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "class Tanh:\n",
        "  def __call__(self, x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return []"
      ],
      "metadata": {
        "id": "CYa9FMaUYOtG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mixed Model"
      ],
      "metadata": {
        "id": "iPMxl7Zu6jiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**mixture model**"
      ],
      "metadata": {
        "id": "dZa606hR8PkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MixedInfiniGram:\n",
        "  def __init__(self, n_embd, n_hidden, block_size, corpus):\n",
        "    self.text = corpus.get_corpus()\n",
        "    self.infinigram = InfiniGram(corpus)\n",
        "    self.mlp = MLP(corpus.get_vocab_size(), n_embd, n_hidden, block_size)\n",
        "\n",
        "  def predict_next(self, prefix, gamma=0.8):\n",
        "    # n-gram can safely search for values not in the corpus.\n",
        "    infinigram_predictions = self.infinigram.predict_next(prefix)\n",
        "\n",
        "    # word_to_ix encoding needs to be checked.\n",
        "    if prefix not in word_to_ix:\n",
        "      prefix = '<UNK>'\n",
        "\n",
        "    x_ix = torch.tensor(word_to_ix[prefix]).unsqueeze(0)\n",
        "    mlp_probs = self.mlp.predict(x_ix)\n",
        "\n",
        "    mixed_probs = gamma * infinigram_predictions + (1 - gamma) * mlp_probs\n",
        "\n",
        "    return [ix_to_word[ix.item()] for ix in torch.topk(mixed_probs, 5).indices]\n",
        "\n",
        "  @property\n",
        "  def layers(self):\n",
        "    return self.mlp.layers\n",
        "\n",
        "  @property\n",
        "  def parameters(self):\n",
        "    return self.mlp.parameters"
      ],
      "metadata": {
        "id": "PeaUP4X9ljij"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "P8O2RBzGc51L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Params\n",
        "text = corpus.get_corpus()\n",
        "vocab = corpus.get_vocab()\n",
        "vocab_size = corpus.get_vocab_size()\n",
        "n_embd = 10 # dimensionality of each char's embedding\n",
        "n_hidden = 100 # the number of neurons in the hidden layer\n",
        "block_size = 1 # number of words in input layer\n",
        "\n",
        "X, Y = build_dataset(words)\n",
        "model = MixedInfiniGram(n_embd, n_hidden, block_size, corpus)"
      ],
      "metadata": {
        "id": "zp_cs880M7bc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Train MLP"
      ],
      "metadata": {
        "id": "6MkdMhOYZAra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_TRAINING_ITERS = 200\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "\n",
        "# batchnorm stuff, maybe remove\n",
        "mu_i = [torch.zeros_like(p) for p in model.parameters]\n",
        "v_i = [torch.zeros_like(p) for p in model.parameters]\n",
        "\n",
        "eps = 1e-8"
      ],
      "metadata": {
        "id": "fmqb6qRrZCFN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_TRAINING_ITERS):\n",
        "  t = epoch + 1\n",
        "  # Construct minibatch\n",
        "  ix = torch.randint(0, X.shape[0], (batch_size,))\n",
        "  Xb, Yb = X[ix], Y[ix]\n",
        "\n",
        "  #forward pass\n",
        "  # --------------------------------------------\n",
        "  emb = model.mlp.C[Xb]\n",
        "  x = emb.view(emb.shape[0], -1) # concatenate embeddigns into n_sample vectors of vocab * dim_size\n",
        "  for layer in model.layers:\n",
        "    x = layer(x)\n",
        "\n",
        "  loss = F.cross_entropy(x, Yb) # loss function\n",
        "\n",
        "  # loss = model(Xb, Yb)\n",
        "\n",
        "  # model.zero_grads()\n",
        "  for layer in model.layers:\n",
        "    layer.out.retain_grad()\n",
        "\n",
        "  for p in model.parameters:\n",
        "    p.grad = None\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  # lr = 0.1 if i < 100000 else 0.01\n",
        "  lr = 0.001\n",
        "  for i, p in enumerate(model.parameters):\n",
        "    mt = beta1 * mu_i[i] + (1-beta1) * p.grad\n",
        "    vt = beta2 * v_i[i] + (1-beta2) * p.grad**2\n",
        "\n",
        "    mu_i[i] = mt # setting moving average to latest average\n",
        "    v_i[i] = vt\n",
        "\n",
        "    # Correct terms\n",
        "    muhat = mt / (1 - beta1**t)\n",
        "    vhat = vt / (1 - beta2**t)\n",
        "\n",
        "    denom = torch.sqrt(vhat) + eps\n",
        "    update = muhat / denom\n",
        "    # p.data += -lr * p.grad\n",
        "    p.data += -lr * (muhat / (vhat**0.5 + eps))\n",
        "\n",
        "  if t % 10000 == 0:\n",
        "    print(f\"{t:7d}/{NUM_TRAINING_ITERS}: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "  # Track stats\n",
        "  lossi.append(loss.log10().item())"
      ],
      "metadata": {
        "id": "N_wUk0pVZJNi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Predict"
      ],
      "metadata": {
        "id": "RvTawmMA9ch5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict_next(\"von neumann was\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yd1M2JOZKmp",
        "outputId": "f28b8df6-60af-4ad6-fc60-26a7e0f7ae4c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'one', 'finishing', 'descended', 'just']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regex based counter"
      ],
      "metadata": {
        "id": "5e2xE_snQwCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_next_words(prefix, text):\n",
        "  pattern = pattern = re.compile(rf'\\b{re.escape(prefix)}\\s+(\\w+)', re.IGNORECASE)\n",
        "  matches = pattern.findall(text)\n",
        "  return Counter(matches)\n",
        "\n",
        "count_next_words('von neumann was', corpus.get_corpus()).most_common()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErF_aq8MQyOE",
        "outputId": "eed88c15-1d7b-42d7-d062-00211afc8f40"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('descended', 1), ('just', 1), ('finishing', 1), ('one', 1), ('a', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}