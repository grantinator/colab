{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzHElT/1Q6Ci0QeXgTssvJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grantinator/colab/blob/main/infini_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset (names dataset)"
      ],
      "metadata": {
        "id": "wIa9t0dQLuJ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPTQMS2tLoV-",
        "outputId": "bbc2134b-1800-4051-d30b-ecdc50a3dcf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-23 16:55:52--  https://raw.githubusercontent.com/exanova-y/von_neumann_dataset/refs/heads/main/biography.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 747769 (730K) [text/plain]\n",
            "Saving to: ‘corpus.txt’\n",
            "\n",
            "corpus.txt          100%[===================>] 730.24K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-07-23 16:55:53 (19.7 MB/s) - ‘corpus.txt’ saved [747769/747769]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/exanova-y/von_neumann_dataset/refs/heads/main/biography.txt -O corpus.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_corpus = open(\"corpus.txt\", \"r\").read()"
      ],
      "metadata": {
        "id": "Vm_E9cdlNmUr"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Corpus class\n",
        "* Allows you to pull out the first n tokens (words).\n"
      ],
      "metadata": {
        "id": "kRE0gnaeLzNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "class Corpus:\n",
        "  def __init__(self, raw_corpus):\n",
        "    self.raw_corpus = raw_corpus\n",
        "    # maps nth word : index in self.corpus that slices upto and including that word.\n",
        "    self.word_end_index = {}\n",
        "    self.corpus = self._clean()\n",
        "\n",
        "  def _clean(self):\n",
        "    tokens = []\n",
        "    # Break into tokens\n",
        "    for line in raw_corpus.splitlines():\n",
        "      if len(line) == 0:\n",
        "        continue\n",
        "\n",
        "      line = line.split(' ')\n",
        "      tokens.extend(line)\n",
        "\n",
        "    # Clean/normalize individual tokens\n",
        "    cleaned_tokens = []\n",
        "    for i, token in enumerate(tokens):\n",
        "      token = token.lower()\n",
        "      token = token.strip()\n",
        "      # Strip punctuation\n",
        "      token = re.sub(r'[^a-zA-Z]', '', token)\n",
        "\n",
        "      if len(token) > 0:\n",
        "        cleaned_tokens.append(token)\n",
        "\n",
        "    for i, token in enumerate(cleaned_tokens):\n",
        "      if i == 0:\n",
        "        self.word_end_index[i] = len(token) - 1\n",
        "      else:\n",
        "        self.word_end_index[i] = self.word_end_index[i - 1] + len(token) + 1 # count space inbetween.\n",
        "\n",
        "    return ' '.join(cleaned_tokens)\n",
        "\n",
        "  def get_first_n_words(self, n):\n",
        "    return self.corpus[:self.word_end_index[n]+1]\n",
        "\n",
        "  def display(self):\n",
        "\n"
      ],
      "metadata": {
        "id": "63qh8dgAgYJb"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = Corpus(raw_corpus)\n",
        "corpus.get_first_n_words(7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wClhbF53Lyuo",
        "outputId": "527337f0-cef1-4e09-f136-bd6481e69655"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'contents introduction who was john von neumann made'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SuffixArray class\n",
        "* Builds the suffix arrays by mapping `[(suffix, startIndex)]`\n",
        "* For an ngram, finds all (suffix, startIndex) pars where the suffix begins with the given ngram."
      ],
      "metadata": {
        "id": "prBkKEYLu7Qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SuffixArray:\n",
        "  def __init__(self, text, SUFFIX_LIMIT = None):\n",
        "    self.text = text\n",
        "    self.suffixes = []\n",
        "    self.SUFFIX_LIMIT = SUFFIX_LIMIT\n",
        "    self._build()\n",
        "\n",
        "  def _build(self):\n",
        "    words = self.text.split(' ')\n",
        "    if self.SUFFIX_LIMIT and  self.SUFFIX_LIMIT < len(words):\n",
        "      words = words[-self.SUFFIX_LIMIT:]\n",
        "    suffix = ''\n",
        "    suffixes_with_index = []\n",
        "    n = len(self.text)\n",
        "    for word in words[::-1]:\n",
        "      suffix = word.lower() + ' ' + suffix\n",
        "      suffixes_with_index.append((suffix, n - len(suffix)))\n",
        "\n",
        "    # Sort on the suffix\n",
        "    sorted_suffixes = sorted(suffixes_with_index, key=lambda x: x[0])\n",
        "    self.suffixes = sorted_suffixes\n",
        "\n",
        "\n",
        "  def find_ngram_occurrences(self, ngram):\n",
        "    \"\"\"\n",
        "    Find all suffixes that start with this prefix. Basically a lexicographical search\n",
        "    over suffixes.\n",
        "    \"\"\"\n",
        "    ngram = ngram.lower()\n",
        "    suffixes = self.suffixes # starting indexes\n",
        "    text = self.text\n",
        "\n",
        "    def startsWith(startIndex, prefix):\n",
        "      \"\"\"\n",
        "      Checks if text starting at startIndex starts with the prefix.\n",
        "      This is our main comparator.\n",
        "      \"\"\"\n",
        "      return text[startIndex:].startswith(prefix)\n",
        "\n",
        "    def getLowerBound():\n",
        "      low, high = 0, len(suffixes) - 1\n",
        "\n",
        "      while low <= high:\n",
        "        mid = (low + high) // 2\n",
        "\n",
        "        midSuffix = suffixes[mid][0]\n",
        "\n",
        "        if midSuffix < ngram:\n",
        "          low = mid + 1\n",
        "        else:\n",
        "          high = mid - 1\n",
        "      return low\n",
        "\n",
        "    def getUpperBound():\n",
        "      low, high = 0, len(suffixes) - 1\n",
        "      # Hack from chatgpt. If we just use ngram in the high search then ngram = \"von neumann\"\n",
        "      # we would not count \"von neumann made\" as a match. But instead we add the max ascii char\n",
        "      # so \"von neuamnn + <anything>\" < high_ngram and is counted.\n",
        "      highNgram = ngram + chr(255)\n",
        "\n",
        "      while low <= high:\n",
        "        mid = (low + high) // 2\n",
        "\n",
        "        midSuffix = suffixes[mid][0]\n",
        "\n",
        "        if midSuffix < highNgram:\n",
        "          low = mid + 1\n",
        "        else:\n",
        "          high = mid - 1\n",
        "      return low\n",
        "\n",
        "    upperBound = getUpperBound()\n",
        "    lowerBound = getLowerBound()\n",
        "    return suffixes[lowerBound:upperBound]"
      ],
      "metadata": {
        "id": "rSNB2jmzOT3S"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## InfiniGram class\n",
        "* Returns a Counter for next word given a prefix.\n"
      ],
      "metadata": {
        "id": "XgGahTn1vV8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "class InfiniGram:\n",
        "  def __init__(self, text):\n",
        "    self.text = text\n",
        "    self.suffixArray = SuffixArray(text)\n",
        "\n",
        "\n",
        "  def predict_next(self, prefix, top_k=1):\n",
        "    suffixes = self.suffixArray.find_ngram_occurrences(prefix)\n",
        "\n",
        "    candidates = []\n",
        "\n",
        "    for suffixText, startIndex in suffixes:\n",
        "      suffixEndIndex = startIndex + len(prefix) + 1\n",
        "      remainingText = self.text[suffixEndIndex: ]\n",
        "      # If suffix was final sentence\n",
        "      if not remainingText:\n",
        "        continue\n",
        "\n",
        "      nextWord = remainingText.lstrip().split(\" \")[0]\n",
        "      candidates.append(nextWord)\n",
        "    counts = Counter(candidates)\n",
        "\n",
        "    if top_k == 1:\n",
        "      # Get the most common. If ties just get the first result.\n",
        "      return counts.most_common(1)[0][0]\n",
        "\n",
        "    return counts.most_common(top_k)"
      ],
      "metadata": {
        "id": "8o4nSck2qP4Y"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ig = InfiniGram(corpus.get_first_n_words(200))"
      ],
      "metadata": {
        "id": "2NdAJj9mr3SF"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(corpus.get_first_n_words(200))"
      ],
      "metadata": {
        "id": "Rfc4ajjvsbzK"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ig.predict_next('von neumann', top_k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H58lL_Fr8d0",
        "outputId": "45a52660-4ea8-4c7f-c0b2-240ae7af0bca"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('enjoyed', 1), ('felt', 1), ('made', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ig.predict_next('von neumann')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sYYwRo2MzHKT",
        "outputId": "a0f4e8ac-af3c-4549-8e21-82d2181a1564"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'enjoyed'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    }
  ]
}